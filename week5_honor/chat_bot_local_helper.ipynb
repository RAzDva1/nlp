{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-15 17:16:30--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_conversations.txt\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt [following]\n",
      "--2021-06-15 17:16:31--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6760930 (6.4M) [text/plain]\n",
      "Saving to: 'movie_conversations.txt'\n",
      "\n",
      "movie_conversations 100%[===================>]   6.45M  5.64MB/s    in 1.1s    \n",
      "\n",
      "2021-06-15 17:16:32 (5.64 MB/s) - 'movie_conversations.txt' saved [6760930/6760930]\n",
      "\n",
      "--2021-06-15 17:16:32--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_lines.txt\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt [following]\n",
      "--2021-06-15 17:16:33--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34641919 (33M) [text/plain]\n",
      "Saving to: 'movie_lines.txt'\n",
      "\n",
      "movie_lines.txt     100%[===================>]  33.04M  5.82MB/s    in 5.7s    \n",
      "\n",
      "2021-06-15 17:16:40 (5.79 MB/s) - 'movie_lines.txt' saved [34641919/34641919]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! . ../honor/download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/cornell/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('data/cornell/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences that we will be using to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
       " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
       " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
       " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
       " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
       " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences' ids, which will be processed to become our input and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    line_ = line.split(\" +++$+++ \")\n",
    "    if len(line_) == 5:\n",
    "        id2line[line_[0]] = line_[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "for line in conv_lines:\n",
    "    _line = line.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dialog example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've blown the computer!  Elaine!  Set course change!\n",
      "Set!\n",
      "Now!\n",
      "Compute!\n"
     ]
    }
   ],
   "source": [
    "ID = 1488\n",
    "for i in convs[ID]:\n",
    "    print(id2line[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the sentences into questions (inputs) and answers (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have loaded the data correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare lengths of questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = [clean_text(line) for line in questions]\n",
    "clean_answers = [clean_text(line) for line in answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some of the data to ensure that it has been cleaned well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      "\n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n",
      "\n",
      "you are asking me out  that is so cute that is your name again\n",
      "forget it\n",
      "\n",
      "no no it is my fault  we did not have a proper introduction \n",
      "cameron\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(line.split()) for line in clean_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221616.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.684120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.804262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  221616.000000\n",
       "mean       10.684120\n",
       "std        11.804262\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        13.000000\n",
       "max       319.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "23.0\n",
      "32.0\n",
      "57.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove questions and answers that are shorter than 2 words and longer than 32 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_answers len =  221616\n",
      "short_answers len =  168618\n"
     ]
    }
   ],
   "source": [
    "min_line_length = 2\n",
    "max_line_length = 32\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for question, answers in zip(clean_questions, clean_answers):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length and \\\n",
    "        len(answers.split()) >= min_line_length and len(answers.split()) <= max_line_length:\n",
    "        short_questions.append(question)\n",
    "        short_answers.append(answers)\n",
    "\n",
    "print(\"clean_answers len = \", len(clean_answers))\n",
    "print(\"short_answers len = \", len(short_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of lines we will use with the total number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 168618\n",
      "# of answers: 168618\n",
      "% of data used: 76.09%\n"
     ]
    }
   ],
   "source": [
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for the frequency of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55594\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rare words from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 55594\n",
      "Size of vocab we will use: 13598\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "\n",
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to use a different vocabulary sizes for the source and target text, we can set different threshold values. Nonetheless, we will create dictionaries to provide a unique integer for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the unique tokens to the vocabulary dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to map the unique integers to their respective words. i.e. an inverse dictionary for vocab_to_int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13602\n",
      "13602\n",
      "13602\n",
      "13602\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the end of sentence token to the end of every answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text to integers. Replace any words that are not in the respective vocabulary with \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        ints.append(questions_vocab_to_int[word] if word in questions_vocab_to_int else questions_vocab_to_int['<UNK>'])\n",
    "    questions_int.append(ints)\n",
    "\n",
    "answers_int = []\n",
    "for question in short_answers:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        ints.append(answers_vocab_to_int[word] if word in answers_vocab_to_int else answers_vocab_to_int['<UNK>'])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168618\n",
      "168618\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_int))\n",
    "print(len(answers_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate what percentage of all words have been replaced with UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 3404792\n",
      "Number of times <UNK> is used: 89685\n",
      "Percent of words that are <UNK>: 2.63%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int['<UNK>']:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sort questions and answers by the length of questions.  \n",
    " This will reduce the amount of padding during training  \n",
    " Which should speed up training and help to reduce the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168618\n",
      "168618\n",
      "\n",
      "[717, 3866]\n",
      "[2904, 4091, 4091, 4091, 11050, 8057, 6775, 2904, 6587, 2552, 13425, 744, 6476, 2314, 13600]\n",
      "\n",
      "[2904, 5573]\n",
      "[9478, 4689, 13222, 9585, 8057, 12996, 7777, 13601, 679, 9478, 13600]\n",
      "\n",
      "[8071, 2965]\n",
      "[1155, 1546, 10354, 5389, 9478, 11023, 9585, 2239, 13600]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(self):\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    self.input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    self.targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    # Placeholder for a dropout keep probability.\n",
    "    self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionModel.__model_inputs = classmethod(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(self, target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    self.dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionModel.__process_encoding_input = classmethod(process_encoding_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strided_slice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(self, rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    '''Create the encoding layer'''\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=self.keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop]*num_layers)\n",
    "    _, self.enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n",
    "                                                   cell_bw=enc_cell,\n",
    "                                                   sequence_length=sequence_length,\n",
    "                                                   inputs=rnn_inputs, \n",
    "                                                   dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionModel.__encoding_layer = classmethod(encoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13602\n",
      "13602\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = len(vocab)\n",
    "source_vocab_size = len(vocab)\n",
    "vocab_size = len(answers_int_to_vocab)+1\n",
    "embed_size = 1024\n",
    "rnn_size = 1024  \n",
    "batch_size = 32\n",
    "num_layers =  3\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.99\n",
    "min_lr = 0.0001\n",
    "keep_prob = 0.5\n",
    "epochs=50\n",
    "DISPLAY_STEP=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(rnn_size, keep_prob, reuse=False):\n",
    "    lstm =tf.nn.rnn_cell.LSTMCell(rnn_size,reuse=reuse)\n",
    "    drop =tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(rnn_size, encoder_outputs, target_sequence_length, dec_cell):\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size*2,encoder_outputs,\n",
    "                                                                   memory_sequence_length=target_sequence_length)\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n",
    "                                                             attention_layer_size=rnn_size/2)\n",
    "    return attention_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "target_data = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "input_data_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_len')\n",
    "target_data_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='target_len')\n",
    "lr_rate = tf.placeholder(tf.float32, shape=[], name='lr')\n",
    "keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size, embed_size], -1, 1))\n",
    "encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional_dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_cells = lstm(rnn_size, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,encoder_bw_outputs),\n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked_cells, \n",
    "                                                                 cell_bw=stacked_cells, \n",
    "                                                                 inputs=encoder_embedded, \n",
    "                                                                 sequence_length=input_data_len, \n",
    "                                                                 dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs,encoder_bw_outputs),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, ?, 2048) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state_c = tf.concat((encoder_fw_final_state.c,encoder_bw_final_state.c),1)\n",
    "encoder_state_h = tf.concat((encoder_fw_final_state.h,encoder_bw_final_state.h),1)\n",
    "encoder_states = tf.nn.rnn_cell.LSTMStateTuple(c=encoder_state_c,h=encoder_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(?, 2048) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(?, 2048) dtype=float32>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strided_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1],questions_vocab_to_int['<GO>']), main], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, embed_size], -1, 1))\n",
    "decoder_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_cell = lstm(rnn_size*2,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.rnn_cell_impl.DropoutWrapper at 0x7fb1ffa4bac8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer for decoder\n",
    "dense_layer = tf.layers.Dense(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class TrainingHelper\n",
    "\n",
    "A helper for use during training. Only reads inputs.  \n",
    "Returned sample_ids are the argmax of the RNN output logits  \n",
    "dec_cell_inputs=>inputs: A (structure of) input tensors.  \n",
    "target_data_len=>sequence_length: An int32 vector tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "sampler = tf.contrib.seq2seq.TrainingHelper(decoder_embedded, target_data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell state initializer\n",
    "\n",
    "#### attention_cell.zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_cell = attention(rnn_size, encoder_outputs, target_data_len, dec_cell)\n",
    "state = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "state = state.clone(cell_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class BasicDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_train = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell ,helper=sampler,\n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class dynamic_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_train, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_train,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search\n",
    "\n",
    "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence.\n",
    "\n",
    "This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class GreedyEmbeddingHelper\n",
    "\n",
    "A helper for use during inference.\n",
    "\n",
    "Uses the argmax of the output (treated as logits) and passes the result through an embedding layer to get the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                        tf.fill([batch_size],\n",
    "                                                                answers_vocab_to_int['<GO>']),\n",
    "                                                        answers_vocab_to_int['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_infer = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell ,helper=infer_helper,\n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_infer, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_infer,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identity\n",
    "\n",
    "Return a tensor with the same shape and contents as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logits = tf.identity(outputs_train.rnn_output, name='logits')\n",
    "inference_logits = tf.identity(outputs_infer.sample_id, name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Masking\n",
    "\n",
    "Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = tf.sequence_mask(target_data_len, tf.reduce_max(target_data_len), dtype=tf.float32, name='masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class sequence_loss\n",
    "\n",
    "Weighted cross-entropy loss for a sequence of logits.\n",
    "\n",
    "logits: A Tensor of shape [batch_size, sequence_length, num_decoder_symbols] and dtype float. The logits correspond to the prediction across all classes at each timestep.\n",
    "\n",
    "targets: A Tensor of shape [batch_size, sequence_length] and dtype int. The target represents the true class at each timestep.\n",
    "\n",
    "weights: A Tensor of shape [batch_size, sequence_length] and dtype float. weights constitutes the weighting of each prediction in the sequence. When using weights as masking, set all valid timesteps to 1 and all padded timesteps to 0, e.g. a mask returned by tf.sequence_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.contrib.seq2seq.sequence_loss(training_logits,target_data,masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output. This type of learning algorithm is designed based on the way neurons function in the human brain. There are many ways to compute gradient clipping, but a common one is to rescale gradients so that their norm is at most a particular value. With gradient clipping, pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm. This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped. There is an introduced bias in the resulting values from the gradient, but gradient clipping can keep things stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip_by_value\n",
    "\n",
    "Clips tensor values to a specified min and max.\n",
    "\n",
    "Compute gradients of loss for the variables in var_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Before training, we work on the dataset to convert the variable length sequences into fixed length sequences, by padding. We use a few special symbols to fill in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy computation\n",
    "\n",
    "np.pad will take the input array and add the padding based on the shape\n",
    "\n",
    "np.equalcompare the target and prediction elementwise\n",
    "\n",
    "np.meanAverage of the inputs given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    max_seq = max(len(target[1]), logits.shape[1])\n",
    "    if max_seq - len(target[1]):\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - len(target[1]))],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test data split\n",
    "\n",
    "As we know input and output will be our questions and answers.Here we are splliting our dataset wrt batch size(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = questions_int[batch_size:]\n",
    "test_data = answers_int[batch_size:]\n",
    "val_train_data = questions_int[:batch_size]\n",
    "val_test_data = answers_int[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168586"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train test validation set\n",
    "\n",
    "we need to pad the sentence before use it to validate our model as we seen in padding section. As our vocabulary index already has the word 'PAD' it.checking it us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_int = questions_vocab_to_int['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_x,val_batch_len = pad_sentence(val_train_data,pad_int)\n",
    "val_batch_y,val_batch_len_y = pad_sentence(val_test_data,pad_int)\n",
    "val_batch_x = np.array(val_batch_x)\n",
    "val_batch_y = np.array(val_batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round the length of train data\n",
    "\n",
    "we need to round the length of train data wrt batch size in order to have equal number of sentence in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_batches = math.floor(len(train_data)//batch_size)\n",
    "round_no = no_of_batches*batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence to sequence\n",
    "\n",
    "So as given below if we have a question sentence 'how are you' it must not be given as it is in our rnn it must be converted into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocabs_to_index):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocabs_to_index:\n",
    "            results.append(vocabs_to_index[word])\n",
    "        else:\n",
    "            results.append(vocabs_to_index['<UNK>'])        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7686, 11266, 9478]\n"
     ]
    }
   ],
   "source": [
    "question_sentence = 'where are you'\n",
    "question_sentence = sentence_to_seq(question_sentence, questions_vocab_to_int)\n",
    "print(question_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf Session Run/Train Model\n",
    "\n",
    "Only after running tf.global_variables_initializer() in a session our variables hold the values we told them to hold when we declare them (tf.Variable(tf.zeros(...)), tf.Variable(tf.random_normal(...)),...).\n",
    "\n",
    "#### Session\n",
    "\n",
    "we need to run the session by providing the optimize which compute gradient wrt cost in each step and all input and target data with its length. It can return prediction of input data and it must be compared with the original target data to get the accuracy of our model.and which will be cumulated further to get total accuracy for all the batches in a single epochs. Also return the loss for each batch which will be cumulated further to get total loss for all the batches in a single epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "target_data = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "# input_data_len = tf.placeholder(tf.int32, [None], name='input_len')\n",
    "input_data_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_len')\n",
    "# target_data_len = tf.placeholder(tf.int32, [None], name='target_len')\n",
    "target_data_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='target_len')\n",
    "lr_rate = tf.placeholder(tf.float32, shape=[], name='lr')\n",
    "keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Average_loss 0.002074, Average Accucracy 0.000000\n",
      "  Inputs Words: ['where', 'are', 'you']\n",
      "  Replied Words: i i i\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = '/root/coursera/natural-language-processing/week5_honor/model_weights/model'\n",
    "acc_plt = []\n",
    "loss_plt = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "    #for epoch in range(1):\n",
    "        #_, summaries_str = sess.run([train_op, summaries_op])\n",
    "        #fw.add_summary(summaries_str, global_step=i)\n",
    "        total_accuracy = 0.0\n",
    "        total_loss = 0.0\n",
    "        for bs in tqdm(range(0,round_no  ,batch_size)):\n",
    "        #for bs in tqdm(range(0,32  ,batch_size)):\n",
    "            index = min(bs+batch_size, round_no )\n",
    "            #print(bs,index)\n",
    "      \n",
    "            #padding done seperately for each batch in training and testing data\n",
    "            batch_x,len_x = pad_sentence(train_data[bs:index],pad_int)\n",
    "            batch_y,len_y = pad_sentence(test_data[bs:index],pad_int)\n",
    "            batch_x = np.array(batch_x)\n",
    "            batch_y = np.array(batch_y)\n",
    "            \n",
    "            pred,loss_f,opt = sess.run([inference_logits,cost,train_op], \n",
    "                                      feed_dict={input_data: batch_x,\n",
    "                                                 target_data: batch_y,\n",
    "                                                 input_data_len: len_x,\n",
    "                                                 target_data_len: len_y,\n",
    "                                                 lr_rate: learning_rate,\n",
    "                                                 keep_prob: 0.5})\n",
    "            train_acc = get_accuracy(batch_y, pred)\n",
    "            total_loss += loss_f \n",
    "            total_accuracy+=train_acc\n",
    "    \n",
    "        total_accuracy /= (round_no // batch_size)\n",
    "    \n",
    "        total_loss /=  (round_no//batch_size)\n",
    "        acc_plt.append(total_accuracy)\n",
    "        loss_plt.append(total_loss)\n",
    "        prediction_logits = sess.run(inference_logits, {input_data: [question_sentence]*batch_size,\n",
    "                                         input_data_len: [len(question_sentence)]*batch_size,\n",
    "                                         target_data_len: [len(question_sentence)]*batch_size,              \n",
    "                                         keep_prob: 0.75,\n",
    "                                         })[0]\n",
    "        print('Epoch %d,Average_loss %f, Average Accucracy %f'%(epoch+1,total_loss,total_accuracy))\n",
    "        print('  Inputs Words: {}'.format([answers_int_to_vocab[i] for i in question_sentence]))\n",
    "        print('  Replied Words: {}'.format(\" \".join([answers_int_to_vocab[i] for i in prediction_logits])))\n",
    "        print('\\n')\n",
    "        saver = tf.train.Saver() \n",
    "        saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Collecting tensorflow-tensorboard==0.4\n",
      "  Downloading tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |################################| 1.7 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (0.9999999)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (1.11.0)\n",
      "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (1.5.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (0.12.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow-tensorboard==0.4) (0.29.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (3.5.0.post1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (2.6.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard==0.4) (1.13.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.4.0->tensorflow-tensorboard==0.4) (20.7.0)\n",
      "Installing collected packages: tensorflow-tensorboard\n",
      "  Attempting uninstall: tensorflow-tensorboard\n",
      "    Found existing installation: tensorflow-tensorboard 1.5.1\n",
      "    Uninstalling tensorflow-tensorboard-1.5.1:\n",
      "      Successfully uninstalled tensorflow-tensorboard-1.5.1\n",
      "Successfully installed tensorflow-tensorboard-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-tensorboard==0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
